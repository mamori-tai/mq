{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home mq is a publish/subscribe (or queueing system) mechanism based on mongodb . It allows to launch small tasks in an asynchronous way. Several tools exist but using other datastore such as Redis (rq, ...) so if you already use redis, it may be a better choice. mq is a hobby project mainly to learn asyncio module, and multiprocessing condition primitives. Getting started \ud83d\ude80 pip poetry pip install mq poetry add mq How it works \u2049 mq can work in several ways. Usually, we enqueue tasks in a process, and launch another process to dequeue these tasks and perform their execution. mq support this in several ways: launch worker process in same script becoming a subprocess of the main process launch worker process in another script. launch worker in a thread for heavy IO tasks. Examples \ud83c\udfa8 minimal example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import random import asyncio from functools import partial from loguru import logger from mq.mq import mq , job @job ( channel = \"test\" ) async def job_test ( a , b ): await asyncio . sleep ( 1 ) return a + b async def main (): await mq . init ( mongo_uri = \"mongodb://localhost:27017\" , db_name = \"mq\" , collection = \"mq\" , ) # start worker process in same process mq . worker_for ( \"default\" ) . start () # enqueue job, coroutine function r1 , r2 = random . randint ( 1 , 100 ), random . randint ( 1 , 100 ) job_result = await job_test . mq ( r1 , r2 ) # add a callback when it' s done ! job_result . add_done_callback ( partial ( logger . debug , \"Got a result {} !\" )) if __name == \"__main__\" : asyncio . run ( main ()) You can also simply wait for the job result await job_result . wait_for_result ( timeout = None ) Or cancel the job while it is running: await job_result . cancel ( timeout = None ) Launch worker process in another script Needs to add a parameter in the init function of mq, in the enqueuing process await mq . with_process_connection ( # specify connection parameters to the main manager process MQManagerConnectionParameters ( url = \"127.0.0.1\" , port = 50000 , authkey = b \"abracadabra\" ) ) . init ( # specify mongodb connection MongoDBConnectionParameters ( mongo_uri = \"mongodb://localhost:27017\" , db_name = \"mq\" , collection = \"mq\" , ), # add this one to start manager server start_server = True , ) Now in another script: await mq . with_process_connection ( # specify connection parameters to the main manager process MQManagerConnectionParameters ( url = \"127.0.0.1\" , port = 50000 , authkey = b \"abracadabra\" ) #(1) ) . init ( # specify mongodb connection MongoDBConnectionParameters ( mongo_uri = \"mongodb://localhost:27017\" , db_name = \"mq\" , collection = \"mq\" , ), in_worker_process = True , ) Needs to connect to other process and acceed to shared memory. Process can also be remote \u2601 !","title":"Tutorial"},{"location":"#home","text":"mq is a publish/subscribe (or queueing system) mechanism based on mongodb . It allows to launch small tasks in an asynchronous way. Several tools exist but using other datastore such as Redis (rq, ...) so if you already use redis, it may be a better choice. mq is a hobby project mainly to learn asyncio module, and multiprocessing condition primitives.","title":"Home"},{"location":"#getting-started","text":"pip poetry pip install mq poetry add mq","title":"Getting started \ud83d\ude80"},{"location":"#how-it-works","text":"mq can work in several ways. Usually, we enqueue tasks in a process, and launch another process to dequeue these tasks and perform their execution. mq support this in several ways: launch worker process in same script becoming a subprocess of the main process launch worker process in another script. launch worker in a thread for heavy IO tasks.","title":"How it works \u2049"},{"location":"#examples","text":"minimal example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import random import asyncio from functools import partial from loguru import logger from mq.mq import mq , job @job ( channel = \"test\" ) async def job_test ( a , b ): await asyncio . sleep ( 1 ) return a + b async def main (): await mq . init ( mongo_uri = \"mongodb://localhost:27017\" , db_name = \"mq\" , collection = \"mq\" , ) # start worker process in same process mq . worker_for ( \"default\" ) . start () # enqueue job, coroutine function r1 , r2 = random . randint ( 1 , 100 ), random . randint ( 1 , 100 ) job_result = await job_test . mq ( r1 , r2 ) # add a callback when it' s done ! job_result . add_done_callback ( partial ( logger . debug , \"Got a result {} !\" )) if __name == \"__main__\" : asyncio . run ( main ()) You can also simply wait for the job result await job_result . wait_for_result ( timeout = None ) Or cancel the job while it is running: await job_result . cancel ( timeout = None )","title":"Examples \ud83c\udfa8"},{"location":"#launch-worker-process-in-another-script","text":"Needs to add a parameter in the init function of mq, in the enqueuing process await mq . with_process_connection ( # specify connection parameters to the main manager process MQManagerConnectionParameters ( url = \"127.0.0.1\" , port = 50000 , authkey = b \"abracadabra\" ) ) . init ( # specify mongodb connection MongoDBConnectionParameters ( mongo_uri = \"mongodb://localhost:27017\" , db_name = \"mq\" , collection = \"mq\" , ), # add this one to start manager server start_server = True , ) Now in another script: await mq . with_process_connection ( # specify connection parameters to the main manager process MQManagerConnectionParameters ( url = \"127.0.0.1\" , port = 50000 , authkey = b \"abracadabra\" ) #(1) ) . init ( # specify mongodb connection MongoDBConnectionParameters ( mongo_uri = \"mongodb://localhost:27017\" , db_name = \"mq\" , collection = \"mq\" , ), in_worker_process = True , ) Needs to connect to other process and acceed to shared memory. Process can also be remote \u2601 !","title":"Launch worker process in another script"},{"location":"dequeuing/","text":"Dequeuing Mq provides several dequeuing mechanisms. Could be in a subprocess, completely in another process or finally in thread for IO heavy tasks. Launching a worker subprocess Launching a thread process Launching worker in another process Specifying a task runner for channel @register_task_runner def print_payload ( current_job ): logger . debug ( current_job . payload )","title":"Dequeuing"},{"location":"dequeuing/#dequeuing","text":"Mq provides several dequeuing mechanisms. Could be in a subprocess, completely in another process or finally in thread for IO heavy tasks.","title":"Dequeuing"},{"location":"dequeuing/#launching-a-worker-subprocess","text":"","title":"Launching a worker subprocess"},{"location":"dequeuing/#launching-a-thread-process","text":"","title":"Launching a thread process"},{"location":"dequeuing/#launching-worker-in-another-process","text":"","title":"Launching worker in another process"},{"location":"dequeuing/#specifying-a-task-runner-for-channel","text":"@register_task_runner def print_payload ( current_job ): logger . debug ( current_job . payload )","title":"Specifying a task runner for channel"},{"location":"enqueuing/","text":"Enqueueing You can customize the way job are enqueued and executed. You can pass several interesting arguments such as: schedule retry stop wait downstream Scheduling jobs \u23f1 For scheduling, we use the amazing schedule library. When enqueuing a job, you can specify the schedule keyword. @job ( channel = \"main\" , schedule = every ( 10 ) . seconds ) async def add_job ( a : int , b : int ) -> int : await asyncio . sleep ( 1 ) return a + b Retrying policy \u25c0 The keyword retry, stop and wait are handled by tenacity library. @job ( channel = \"main\" , stop = stop_after_attempt ( 3 )) async def add_job ( a : int , b : int ) -> int : await asyncio . sleep ( 1 ) return a + b To see tenacity in action, do not hesitate to visit their documentation ! Downstream \u267b Very often you want to define relation between tasks (DAG or directed acyclic graph ). For this use case, you can use the downstream keyword argument import asyncio from mq import job @job ( channel = \"main\" ) async def add_100 ( a : int ) -> int : return a + 100 @job ( channel = \"main\" , downstream = [ add_100 ]) async def add_job ( a : int , b : int ) -> int : await asyncio . sleep ( 1 ) return a + b Note A task cancellation leads to cancellation of all downstream tasks Getting child jobs result To get result of a downstream job, you can do the following ( leaves correspond to job id of the last task of the flow): async def main (): # enqueue job... command = await add_job . mq ( 1 , 2 ) # get the child job child_job_id = await command . leaves ()[ 0 ] child_command = command . command_for ( child_job_id ) result = await child_command . wait_for_result ()","title":"Enqueueing"},{"location":"enqueuing/#enqueueing","text":"You can customize the way job are enqueued and executed. You can pass several interesting arguments such as: schedule retry stop wait downstream","title":"Enqueueing"},{"location":"enqueuing/#scheduling-jobs","text":"For scheduling, we use the amazing schedule library. When enqueuing a job, you can specify the schedule keyword. @job ( channel = \"main\" , schedule = every ( 10 ) . seconds ) async def add_job ( a : int , b : int ) -> int : await asyncio . sleep ( 1 ) return a + b","title":"Scheduling jobs \u23f1"},{"location":"enqueuing/#retrying-policy","text":"The keyword retry, stop and wait are handled by tenacity library. @job ( channel = \"main\" , stop = stop_after_attempt ( 3 )) async def add_job ( a : int , b : int ) -> int : await asyncio . sleep ( 1 ) return a + b To see tenacity in action, do not hesitate to visit their documentation !","title":"Retrying policy \u25c0"},{"location":"enqueuing/#downstream","text":"Very often you want to define relation between tasks (DAG or directed acyclic graph ). For this use case, you can use the downstream keyword argument import asyncio from mq import job @job ( channel = \"main\" ) async def add_100 ( a : int ) -> int : return a + 100 @job ( channel = \"main\" , downstream = [ add_100 ]) async def add_job ( a : int , b : int ) -> int : await asyncio . sleep ( 1 ) return a + b Note A task cancellation leads to cancellation of all downstream tasks","title":"Downstream \u267b"},{"location":"enqueuing/#getting-child-jobs-result","text":"To get result of a downstream job, you can do the following ( leaves correspond to job id of the last task of the flow): async def main (): # enqueue job... command = await add_job . mq ( 1 , 2 ) # get the child job child_job_id = await command . leaves ()[ 0 ] child_command = command . command_for ( child_job_id ) result = await child_command . wait_for_result ()","title":"Getting child jobs result"},{"location":"api/job-command/","text":"Bases: CancelDownstreamJobMixin Provides simple command to interact with enqueued job Source code in mq/_queue.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 class JobCommand ( CancelDownstreamJobMixin ): \"\"\" Provides simple command to interact with enqueued job \"\"\" def __init__ ( self , job_id : str , q : AsyncIOMotorCollection , events : dict [ str , list [ threading . Event ]], ): self . _job_id = job_id self . q = q self . events = events self . _result = None self . _tasks = set () @property def job_id ( self ): return self . _job_id async def job ( self , as_job : bool = False ) -> dict [ str , Any ] | Job : job_as_dict = await self . q . find_one ({ \"_id\" : self . _job_id }) if as_job : return Job ( ** job_as_dict ) return job_as_dict async def delete ( self ) -> DeleteResult | typing . NoReturn : \"\"\" Allow deleting the job and associated events in shared memory Returns: \"\"\" job = await self . job ( as_job = True ) if job . status not in { JobStatus . CANCELLED , JobStatus . ON_ERROR , JobStatus . FINISHED , }: raise DeleteJobError ( f \"Job id { job . id } in status { job . status } \" ) # delete from all events del self . events [ job . id ] # delete from database return await self . q . delete_one ({ \"_id\" : job . id }) def command_for ( self , downstream_id : str ) -> \"JobCommand\" : \"\"\" Returns a JobCommand for a downstream job to perform operations on it Args: downstream_id: str the id of the downstream job Returns: JobCommand \"\"\" return JobCommand ( downstream_id , self . q , self . events ) async def leaves ( self , leaves = None ): \"\"\" return all leaves i.e. job id of a Directed Acyclic Graph Args: leaves: None (used in recursive manner) Returns: list[str] all job_id of job leaves \"\"\" if leaves is None : leaves = [] job_as_dict = await self . job ( as_job = True ) jobs = job_as_dict . computed_downstream for v , child in jobs . items (): if not child : leaves . append ( v ) job_command = JobCommand ( v , self . q , self . events ) await job_command . leaves ( leaves ) return leaves async def cancel ( self ) -> bool : \"\"\" Try to cancel a job even if ti is running Returns: bool cancelling success \"\"\" # retrieving cancel condition ev_result , ev_cancel = self . events . get ( self . _job_id ) ev_cancel . set () logger . debug ( \"Cancelling downstream job...\" ) await self . cancel_downstream ( computed_downstream = ( await self . job ())[ \"computed_downstream\" ] ) # waiting to finish await asyncio . to_thread ( partial ( ev_result . wait , 1 )) return ( await self . job ())[ \"status\" ] == JobStatus . CANCELLED async def wait_for_result ( self , timeout : float | None = None ) -> Any : \"\"\" wait for the result of the job Args: timeout: float time to wait after returning. If None wait forever Returns: result: Any the result of the job \"\"\" event_result , event_cancel = self . events . get ( self . _job_id ) if event_cancel . is_set (): raise JobCancelledError ( f \"Job id $ { self . _job_id } has been cancelled\" ) refreshed_job = await self . job () if refreshed_job [ \"status\" ] == JobStatus . CANCELLED : raise JobCancelledError ( f \"Job id $ { refreshed_job [ '_id' ] } has been cancelled\" ) event = self . events . get ( self . _job_id )[ 0 ] if event is None : raise ValueError ( \"Could not find event\" ) executor = concurrent . futures . ThreadPoolExecutor ( max_workers = 1 ) try : async with async_timeout . timeout ( timeout ): await asyncio . get_running_loop () . run_in_executor ( executor , event . wait ) except asyncio . TimeoutError : return None finally : executor . shutdown () refreshed_job = await self . job () if ( result := refreshed_job . get ( \"result\" )) is None : return None self . _result = loads ( result ) return self . _result def _done_cb ( self , task , cb ): self . _tasks . discard ( task ) try : return cb ( task . result ()) except CancelledError : return cb ( None ) def add_done_callback ( self , cb : Callable | Coroutine ): \"\"\" Add a callback when a job is done (even if it failed) Args: cb: Callable | Coroutine Returns: None \"\"\" task = asyncio . get_running_loop () . create_task ( self . wait_for_result ()) task . add_done_callback ( lambda t : self . _done_cb ( t , cb )) add_done_callback ( cb ) Add a callback when a job is done (even if it failed) Parameters: Name Type Description Default cb Callable | Coroutine Callable | Coroutine required Returns: Type Description None Source code in mq/_queue.py 180 181 182 183 184 185 186 187 188 189 190 def add_done_callback ( self , cb : Callable | Coroutine ): \"\"\" Add a callback when a job is done (even if it failed) Args: cb: Callable | Coroutine Returns: None \"\"\" task = asyncio . get_running_loop () . create_task ( self . wait_for_result ()) task . add_done_callback ( lambda t : self . _done_cb ( t , cb )) cancel () async Try to cancel a job even if ti is running Returns: Type Description bool bool cancelling success Source code in mq/_queue.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 async def cancel ( self ) -> bool : \"\"\" Try to cancel a job even if ti is running Returns: bool cancelling success \"\"\" # retrieving cancel condition ev_result , ev_cancel = self . events . get ( self . _job_id ) ev_cancel . set () logger . debug ( \"Cancelling downstream job...\" ) await self . cancel_downstream ( computed_downstream = ( await self . job ())[ \"computed_downstream\" ] ) # waiting to finish await asyncio . to_thread ( partial ( ev_result . wait , 1 )) return ( await self . job ())[ \"status\" ] == JobStatus . CANCELLED command_for ( downstream_id ) Returns a JobCommand for a downstream job to perform operations on it Parameters: Name Type Description Default downstream_id str str the id of the downstream job required Returns: Type Description JobCommand JobCommand Source code in mq/_queue.py 86 87 88 89 90 91 92 93 94 95 96 def command_for ( self , downstream_id : str ) -> \"JobCommand\" : \"\"\" Returns a JobCommand for a downstream job to perform operations on it Args: downstream_id: str the id of the downstream job Returns: JobCommand \"\"\" return JobCommand ( downstream_id , self . q , self . events ) delete () async Allow deleting the job and associated events in shared memory Source code in mq/_queue.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 async def delete ( self ) -> DeleteResult | typing . NoReturn : \"\"\" Allow deleting the job and associated events in shared memory Returns: \"\"\" job = await self . job ( as_job = True ) if job . status not in { JobStatus . CANCELLED , JobStatus . ON_ERROR , JobStatus . FINISHED , }: raise DeleteJobError ( f \"Job id { job . id } in status { job . status } \" ) # delete from all events del self . events [ job . id ] # delete from database return await self . q . delete_one ({ \"_id\" : job . id }) leaves ( leaves = None ) async return all leaves i.e. job id of a Directed Acyclic Graph Parameters: Name Type Description Default leaves None (used in recursive manner) None Returns: Type Description list[str] all job_id of job leaves Source code in mq/_queue.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 async def leaves ( self , leaves = None ): \"\"\" return all leaves i.e. job id of a Directed Acyclic Graph Args: leaves: None (used in recursive manner) Returns: list[str] all job_id of job leaves \"\"\" if leaves is None : leaves = [] job_as_dict = await self . job ( as_job = True ) jobs = job_as_dict . computed_downstream for v , child in jobs . items (): if not child : leaves . append ( v ) job_command = JobCommand ( v , self . q , self . events ) await job_command . leaves ( leaves ) return leaves wait_for_result ( timeout = None ) async wait for the result of the job Parameters: Name Type Description Default timeout float | None float time to wait after returning. If None wait forever None Returns: Name Type Description result Any Any the result of the job Source code in mq/_queue.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 async def wait_for_result ( self , timeout : float | None = None ) -> Any : \"\"\" wait for the result of the job Args: timeout: float time to wait after returning. If None wait forever Returns: result: Any the result of the job \"\"\" event_result , event_cancel = self . events . get ( self . _job_id ) if event_cancel . is_set (): raise JobCancelledError ( f \"Job id $ { self . _job_id } has been cancelled\" ) refreshed_job = await self . job () if refreshed_job [ \"status\" ] == JobStatus . CANCELLED : raise JobCancelledError ( f \"Job id $ { refreshed_job [ '_id' ] } has been cancelled\" ) event = self . events . get ( self . _job_id )[ 0 ] if event is None : raise ValueError ( \"Could not find event\" ) executor = concurrent . futures . ThreadPoolExecutor ( max_workers = 1 ) try : async with async_timeout . timeout ( timeout ): await asyncio . get_running_loop () . run_in_executor ( executor , event . wait ) except asyncio . TimeoutError : return None finally : executor . shutdown () refreshed_job = await self . job () if ( result := refreshed_job . get ( \"result\" )) is None : return None self . _result = loads ( result ) return self . _result","title":"Job command"},{"location":"api/job-command/#mq._queue.JobCommand.add_done_callback","text":"Add a callback when a job is done (even if it failed) Parameters: Name Type Description Default cb Callable | Coroutine Callable | Coroutine required Returns: Type Description None Source code in mq/_queue.py 180 181 182 183 184 185 186 187 188 189 190 def add_done_callback ( self , cb : Callable | Coroutine ): \"\"\" Add a callback when a job is done (even if it failed) Args: cb: Callable | Coroutine Returns: None \"\"\" task = asyncio . get_running_loop () . create_task ( self . wait_for_result ()) task . add_done_callback ( lambda t : self . _done_cb ( t , cb ))","title":"add_done_callback()"},{"location":"api/job-command/#mq._queue.JobCommand.cancel","text":"Try to cancel a job even if ti is running Returns: Type Description bool bool cancelling success Source code in mq/_queue.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 async def cancel ( self ) -> bool : \"\"\" Try to cancel a job even if ti is running Returns: bool cancelling success \"\"\" # retrieving cancel condition ev_result , ev_cancel = self . events . get ( self . _job_id ) ev_cancel . set () logger . debug ( \"Cancelling downstream job...\" ) await self . cancel_downstream ( computed_downstream = ( await self . job ())[ \"computed_downstream\" ] ) # waiting to finish await asyncio . to_thread ( partial ( ev_result . wait , 1 )) return ( await self . job ())[ \"status\" ] == JobStatus . CANCELLED","title":"cancel()"},{"location":"api/job-command/#mq._queue.JobCommand.command_for","text":"Returns a JobCommand for a downstream job to perform operations on it Parameters: Name Type Description Default downstream_id str str the id of the downstream job required Returns: Type Description JobCommand JobCommand Source code in mq/_queue.py 86 87 88 89 90 91 92 93 94 95 96 def command_for ( self , downstream_id : str ) -> \"JobCommand\" : \"\"\" Returns a JobCommand for a downstream job to perform operations on it Args: downstream_id: str the id of the downstream job Returns: JobCommand \"\"\" return JobCommand ( downstream_id , self . q , self . events )","title":"command_for()"},{"location":"api/job-command/#mq._queue.JobCommand.delete","text":"Allow deleting the job and associated events in shared memory Source code in mq/_queue.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 async def delete ( self ) -> DeleteResult | typing . NoReturn : \"\"\" Allow deleting the job and associated events in shared memory Returns: \"\"\" job = await self . job ( as_job = True ) if job . status not in { JobStatus . CANCELLED , JobStatus . ON_ERROR , JobStatus . FINISHED , }: raise DeleteJobError ( f \"Job id { job . id } in status { job . status } \" ) # delete from all events del self . events [ job . id ] # delete from database return await self . q . delete_one ({ \"_id\" : job . id })","title":"delete()"},{"location":"api/job-command/#mq._queue.JobCommand.leaves","text":"return all leaves i.e. job id of a Directed Acyclic Graph Parameters: Name Type Description Default leaves None (used in recursive manner) None Returns: Type Description list[str] all job_id of job leaves Source code in mq/_queue.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 async def leaves ( self , leaves = None ): \"\"\" return all leaves i.e. job id of a Directed Acyclic Graph Args: leaves: None (used in recursive manner) Returns: list[str] all job_id of job leaves \"\"\" if leaves is None : leaves = [] job_as_dict = await self . job ( as_job = True ) jobs = job_as_dict . computed_downstream for v , child in jobs . items (): if not child : leaves . append ( v ) job_command = JobCommand ( v , self . q , self . events ) await job_command . leaves ( leaves ) return leaves","title":"leaves()"},{"location":"api/job-command/#mq._queue.JobCommand.wait_for_result","text":"wait for the result of the job Parameters: Name Type Description Default timeout float | None float time to wait after returning. If None wait forever None Returns: Name Type Description result Any Any the result of the job Source code in mq/_queue.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 async def wait_for_result ( self , timeout : float | None = None ) -> Any : \"\"\" wait for the result of the job Args: timeout: float time to wait after returning. If None wait forever Returns: result: Any the result of the job \"\"\" event_result , event_cancel = self . events . get ( self . _job_id ) if event_cancel . is_set (): raise JobCancelledError ( f \"Job id $ { self . _job_id } has been cancelled\" ) refreshed_job = await self . job () if refreshed_job [ \"status\" ] == JobStatus . CANCELLED : raise JobCancelledError ( f \"Job id $ { refreshed_job [ '_id' ] } has been cancelled\" ) event = self . events . get ( self . _job_id )[ 0 ] if event is None : raise ValueError ( \"Could not find event\" ) executor = concurrent . futures . ThreadPoolExecutor ( max_workers = 1 ) try : async with async_timeout . timeout ( timeout ): await asyncio . get_running_loop () . run_in_executor ( executor , event . wait ) except asyncio . TimeoutError : return None finally : executor . shutdown () refreshed_job = await self . job () if ( result := refreshed_job . get ( \"result\" )) is None : return None self . _result = loads ( result ) return self . _result","title":"wait_for_result()"},{"location":"api/job-queue/","text":"Bases: EnqueueMixin Job queue class which enqueues some jobs Source code in mq/_queue.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 class JobQueue ( EnqueueMixin ): \"\"\" Job queue class which enqueues some jobs \"\"\" def __init__ ( self , * , mongodb_connection : MongoDBConnectionParameters , shared_memory : \"P\" = None , scheduler : SchedulerProtocol , ): self . _mongodb_connection = mongodb_connection self . _client = AsyncIOMotorClient ( mongodb_connection . mongo_uri ) self . db = self . _client [ mongodb_connection . db_name ] self . q : AsyncIOMotorCollection = None self . _shared_memory = shared_memory self . scheduler = scheduler async def init ( self ): if not await self . _exists (): await self . _create () self . q = self . db [ self . connection_parameters . collection ] @property def connection_parameters ( self ): \"\"\" Returns: MongoDBConnectionParameters \"\"\" return self . _mongodb_connection async def _create ( self ): collection = self . connection_parameters . collection try : await self . db . create_collection ( collection ) await self . db [ collection ] . create_index ([( \"status\" , pymongo . ASCENDING )]) except CollectionInvalid as e : raise ValueError ( f \"Collection { collection =} already created\" ) from e async def _exists ( self ): return ( self . connection_parameters . collection in await self . db . list_collection_names () ) async def enqueue ( self , f : Callable [ ... , Any ] | Coroutine | None , * args : Any , ** kwargs : Any ) -> JobCommand : \"\"\" Enqueue a function in mongo Args: f: *args: **kwargs: Returns: JobCommand instance \"\"\" events = self . _shared_memory . events () job = await self . enqueue_job ( job_id = None , status = JobStatus . WAITING , downstream_job = {}, events = events , manager = self . _shared_memory . manager , f = ( f , args , kwargs ), ) # returning the job command # noinspection PyProtectedMember return JobCommand ( job_id = job . _id , q = self . q , events = events ) connection_parameters () property Returns: Type Description MongoDBConnectionParameters Source code in mq/_queue.py 217 218 219 220 221 222 223 224 225 @property def connection_parameters ( self ): \"\"\" Returns: MongoDBConnectionParameters \"\"\" return self . _mongodb_connection enqueue ( f , * args , ** kwargs ) async Enqueue a function in mongo Parameters: Name Type Description Default f Callable [..., Any ] | Coroutine | None required *args Any () **kwargs Any {} Returns: Type Description JobCommand JobCommand instance Source code in mq/_queue.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 async def enqueue ( self , f : Callable [ ... , Any ] | Coroutine | None , * args : Any , ** kwargs : Any ) -> JobCommand : \"\"\" Enqueue a function in mongo Args: f: *args: **kwargs: Returns: JobCommand instance \"\"\" events = self . _shared_memory . events () job = await self . enqueue_job ( job_id = None , status = JobStatus . WAITING , downstream_job = {}, events = events , manager = self . _shared_memory . manager , f = ( f , args , kwargs ), ) # returning the job command # noinspection PyProtectedMember return JobCommand ( job_id = job . _id , q = self . q , events = events )","title":"Job queue"},{"location":"api/job-queue/#mq._queue.JobQueue.connection_parameters","text":"Returns: Type Description MongoDBConnectionParameters Source code in mq/_queue.py 217 218 219 220 221 222 223 224 225 @property def connection_parameters ( self ): \"\"\" Returns: MongoDBConnectionParameters \"\"\" return self . _mongodb_connection","title":"connection_parameters()"},{"location":"api/job-queue/#mq._queue.JobQueue.enqueue","text":"Enqueue a function in mongo Parameters: Name Type Description Default f Callable [..., Any ] | Coroutine | None required *args Any () **kwargs Any {} Returns: Type Description JobCommand JobCommand instance Source code in mq/_queue.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 async def enqueue ( self , f : Callable [ ... , Any ] | Coroutine | None , * args : Any , ** kwargs : Any ) -> JobCommand : \"\"\" Enqueue a function in mongo Args: f: *args: **kwargs: Returns: JobCommand instance \"\"\" events = self . _shared_memory . events () job = await self . enqueue_job ( job_id = None , status = JobStatus . WAITING , downstream_job = {}, events = events , manager = self . _shared_memory . manager , f = ( f , args , kwargs ), ) # returning the job command # noinspection PyProtectedMember return JobCommand ( job_id = job . _id , q = self . q , events = events )","title":"enqueue()"},{"location":"api/mq/","text":"MQ Main class default_worker ( * , channel = 'default' , worker_type = WorkerType . PROCESS , nb_processes = 1 , max_concurrency = 3 , dequeuing_delay = 3 , task_runner = None ) Create a new default worker with a default task runner Parameters: Name Type Description Default channel str str the channel to publish the job 'default' nb_processes int int the number of workers 1 max_concurrency int int the number of workers which work concurrently 3 dequeuing_delay int int time to wait before dequeuing a new job 3 task_runner callable the function to execute a job None Returns: Type Description Worker Worker instance enqueue ( f = None , * args , ** kwargs ) async direct enqueuing of a function Parameters: Name Type Description Default f None *args () **kwargs {} events () property Get all shared events init ( mongodb_connection_params , start_server = False , in_worker_process = False , scheduler = DefaultScheduler ()) async Mq initialization method Parameters: Name Type Description Default mongodb_connection_params MongoDBConnectionParameters MongoDBConnectionParams required start_server bool bool true if we need to start a server process in order to start a worker in another process False in_worker_process bool bool true if worker is launched in another process False scheduler SchedulerProtocol SchedulerProtocol Scheduler instance implementation DefaultScheduler() register_worker ( worker ) Register a new worker Parameters: Name Type Description Default worker Worker Worker a created worker required Returns: Type Description None None with_process_connection ( mq_server_params ) Server connection parameters when starting a server or in a worker process Parameters: Name Type Description Default mq_server_params MQManagerConnectionParameters required worker_with_runner ( * , runner , nb_processes = 1 , worker_type = WorkerType . THREAD ) Create a new worker with a special task runner implementation Parameters: Name Type Description Default runner RunnerProtocol TaskRunnerProtocol the runner implementation see DefaultTaskRunner required nb_processes int int nb of processes 1 Returns: Type Description Worker instance of the built worker P Shared memory to handle getting result or cancel job close () close the shared memory manager events () return all shared memory events job Decorator allowing to define job register_task_runner Register a function to dequeue messages from a channel","title":"Mq"},{"location":"api/mq/#mq.mq.MQ","text":"Main class","title":"MQ"},{"location":"api/mq/#mq.mq.MQ.default_worker","text":"Create a new default worker with a default task runner Parameters: Name Type Description Default channel str str the channel to publish the job 'default' nb_processes int int the number of workers 1 max_concurrency int int the number of workers which work concurrently 3 dequeuing_delay int int time to wait before dequeuing a new job 3 task_runner callable the function to execute a job None Returns: Type Description Worker Worker instance","title":"default_worker()"},{"location":"api/mq/#mq.mq.MQ.enqueue","text":"direct enqueuing of a function Parameters: Name Type Description Default f None *args () **kwargs {}","title":"enqueue()"},{"location":"api/mq/#mq.mq.MQ.events","text":"Get all shared events","title":"events()"},{"location":"api/mq/#mq.mq.MQ.init","text":"Mq initialization method Parameters: Name Type Description Default mongodb_connection_params MongoDBConnectionParameters MongoDBConnectionParams required start_server bool bool true if we need to start a server process in order to start a worker in another process False in_worker_process bool bool true if worker is launched in another process False scheduler SchedulerProtocol SchedulerProtocol Scheduler instance implementation DefaultScheduler()","title":"init()"},{"location":"api/mq/#mq.mq.MQ.register_worker","text":"Register a new worker Parameters: Name Type Description Default worker Worker Worker a created worker required Returns: Type Description None None","title":"register_worker()"},{"location":"api/mq/#mq.mq.MQ.with_process_connection","text":"Server connection parameters when starting a server or in a worker process Parameters: Name Type Description Default mq_server_params MQManagerConnectionParameters required","title":"with_process_connection()"},{"location":"api/mq/#mq.mq.MQ.worker_with_runner","text":"Create a new worker with a special task runner implementation Parameters: Name Type Description Default runner RunnerProtocol TaskRunnerProtocol the runner implementation see DefaultTaskRunner required nb_processes int int nb of processes 1 Returns: Type Description Worker instance of the built worker","title":"worker_with_runner()"},{"location":"api/mq/#mq.mq.P","text":"Shared memory to handle getting result or cancel job","title":"P"},{"location":"api/mq/#mq.mq.P.close","text":"close the shared memory manager","title":"close()"},{"location":"api/mq/#mq.mq.P.events","text":"return all shared memory events","title":"events()"},{"location":"api/mq/#mq.mq.job","text":"Decorator allowing to define job","title":"job"},{"location":"api/mq/#mq.mq.register_task_runner","text":"Register a function to dequeue messages from a channel","title":"register_task_runner"}]}